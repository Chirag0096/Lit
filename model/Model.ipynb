{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1z8AkqqicHH",
        "outputId": "a5ac4d67-cec2-4649-ccfa-2aa51e7aef24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJN-hDtlijLA",
        "outputId": "be244ddc-85f0-45ab-d3e1-79134d43991a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n",
            "··········\n",
            " * ngrok tunnel available, access with `ssh root@2.tcp.ngrok.io -p14187`\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "# Open a TCP ngrok tunnel to the SSH server\n",
        "connection_string = ngrok.connect(\"22\", \"tcp\").public_url\n",
        "\n",
        "ssh_url, port = connection_string.strip(\"tcp://\").split(\":\")\n",
        "print(f\" * ngrok tunnel available, access with `ssh root@{ssh_url} -p{port}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc_XcTkXyiO3"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpn2fzJzlyi3"
      },
      "outputs": [],
      "source": [
        "!fuser -k 5000/tcp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9cgNxZYzPDG",
        "outputId": "e7ac6bb3-0be1-43b7-c39c-4b19e796f368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "Pcsq4comjsZZ",
        "outputId": "763d07bb-8a54-4f43-e32c-d2f9ce15c01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n",
            " * ngrok tunnel \"https://3ec9-35-247-156-102.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:13:44] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:13:46] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:14:47] \"\u001b[31m\u001b[1mPOST /send-message HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Received a request\n",
            "Received data: {'query': 'How does this work?'}\n",
            "Received a request\n",
            "Received data: {'message': 'How does this work?'}\n",
            "Generating response...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:15:21] \"POST /send-message HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response generated.\n",
            "Received a request\n",
            "Received data: {'message': 'Can write a code with python adding two numbers'}\n",
            "Generating response...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:16:01] \"POST /send-message HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response generated.\n",
            "Received a request\n",
            "Received data: {'message': 'do you know anything about brain tumour?'}\n",
            "Generating response...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:18:55] \"POST /send-message HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response generated.\n",
            "Received a request\n",
            "Received data: {'message': 'Can you generate me a treatment plan for pituary tumour.'}\n",
            "Generating response...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:20:02] \"POST /send-message HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response generated.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown\n",
        "import PIL.Image\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "\n",
        "GOOGLE_API_KEY = getpass()\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "# Function to format text to Markdown (as a plain string)\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return textwrap.indent(text, '> ', predicate=lambda _: True)  # Return formatted string\n",
        "\n",
        "# from IPython.core.display import Markdown\n",
        "\n",
        "# def to_markdown(text):\n",
        "#     text = text.replace('•', '  *')\n",
        "#     formatted_markdown = Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "#     # Convert Markdown object to HTML and return as a string\n",
        "#     return str(formatted_markdown)\n",
        "\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Start the Flask server\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"Hello from Colab!\"\n",
        "\n",
        "@app.route(\"/send-message\", methods=[\"POST\"])\n",
        "def send_message():\n",
        "    print(\"Received a request\")\n",
        "    data = request.json\n",
        "    print(\"Received data:\", data)\n",
        "\n",
        "    message = data.get(\"message\", \"\")\n",
        "    if not message:\n",
        "        return jsonify({\"error\": \"No message provided\"}), 400\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    response = model.generate_content(message)\n",
        "    print(\"Response generated.\")\n",
        "\n",
        "    response_text = response.text  # Access the response text\n",
        "    if response_text is not None:\n",
        "        markdown_response = to_markdown(response_text)  # Get the formatted Markdown as HTML\n",
        "        return jsonify({\"response\": markdown_response})   # No markdown conversion\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Failed to generate a response\"}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=port)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXZuTh1Yrxf",
        "outputId": "6266a54d-7dd5-465d-a212-1e9ec27b5e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://e413-34-145-35-23.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Sep/2024 10:01:24] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Sep/2024 10:01:25] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# GOOGLE_API_KEY = getpass()\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "# Function to format text to Markdown (as a plain string)\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyAxwu5faTTimSAg6LDP2g1FQlBDNtK_XGU\")\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"Hello from Colab!\"\n",
        "\n",
        "@app.route(\"/send-message\", methods=[\"POST\"])\n",
        "def send_message():\n",
        "    print(\"Received a request\")\n",
        "    data = request.json\n",
        "\n",
        "    message = data.get(\"message\", \"\")\n",
        "    image_base64 = data.get(\"image\", \"\")\n",
        "\n",
        "    if not message and not image_base64:\n",
        "        return jsonify({\"error\": \"No message or image provided\"}), 400\n",
        "\n",
        "    # If there's an image, decode it from base64\n",
        "    if image_base64:\n",
        "        try:\n",
        "            image_data = base64.b64decode(image_base64)\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": f\"Failed to process image: {str(e)}\"}), 400\n",
        "    else:\n",
        "        image = None\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        # Pass both text and image as input to the model, if image exists\n",
        "        inputs = [message]\n",
        "        if image:\n",
        "            inputs.append(image)\n",
        "\n",
        "        # Using `stream=True` to handle the generation in streaming mode\n",
        "        response = model.generate_content(inputs, stream=True)\n",
        "        response.resolve()\n",
        "        context_text = str(response.text)\n",
        "        markdown_response = to_markdown(context_text)  # Format response as Markdown\n",
        "        return jsonify({\"response\": markdown_response})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Failed to generate response: {str(e)}\"}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=port)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoWbZNxc1e-u",
        "outputId": "194afb3e-1f6b-482b-8774-8b989ea391cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po-Uh8kE1fCe",
        "outputId": "947c9978-b83a-4687-f5d0-8a3c87457976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.89-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.89-py3-none-any.whl (871 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.7/871.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.89 ultralytics-thop-2.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW7SUyH91LCI"
      },
      "source": [
        "with object detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPWwtqcp5zEa",
        "outputId": "394e4757-c92c-4006-83eb-1ae98a63cf4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * ngrok tunnel \"https://4127-34-142-204-47.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Sep/2024 14:35:10] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Sep/2024 14:35:12] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the YOLO object detection model\n",
        "inference_ = YOLO(\"/content/drive/MyDrive/WMS/best.pt\")  # Load your custom-trained model\n",
        "\n",
        "# GOOGLE_API_KEY = getpass()\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "# Function to format text to Markdown (as a plain string)\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyBzOy5pGfDWfgTkd7pVgDRQJ5U35EXDvi8\")\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"Hello from Colab!\"\n",
        "\n",
        "# Object detection function\n",
        "@app.route(\"/object-detection\", methods=[\"POST\"])\n",
        "def object_detection():\n",
        "    print(\"Received a request for object detection\")\n",
        "    data = request.json\n",
        "\n",
        "    image_base64 = data.get(\"image\", \"\")\n",
        "\n",
        "    if not image_base64:\n",
        "        return jsonify({\"error\": \"No image provided\"}), 400\n",
        "\n",
        "    try:\n",
        "        # Decode the image from base64\n",
        "        image_data = base64.b64decode(image_base64)\n",
        "        image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "        # Convert PIL image to numpy array\n",
        "        img_np = np.array(image)\n",
        "\n",
        "        # Apply YOLO object detection\n",
        "        results = inference_([img_np])\n",
        "\n",
        "        # Extract the prediction and plot the results\n",
        "        for r in results:\n",
        "            im_array = r.plot()  # BGR numpy array of predictions\n",
        "            im = Image.fromarray(im_array[..., ::-1])  # Convert to RGB PIL image\n",
        "\n",
        "        # Convert the output image back to base64 to send it in response\n",
        "        buffered = io.BytesIO()\n",
        "        im.save(buffered, format=\"JPEG\")\n",
        "        img_base64_output = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        return jsonify({\"detected_image\": img_base64_output})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Failed to process image for object detection: {str(e)}\"}), 500\n",
        "\n",
        "@app.route(\"/send-message\", methods=[\"POST\"])\n",
        "def send_message():\n",
        "    print(\"Received a request\")\n",
        "    data = request.json\n",
        "\n",
        "    message = data.get(\"message\", \"\")\n",
        "    image_base64 = data.get(\"image\", \"\")\n",
        "\n",
        "    if not message and not image_base64:\n",
        "        return jsonify({\"error\": \"No message or image provided\"}), 400\n",
        "\n",
        "    # If there's an image, decode it from base64\n",
        "    if image_base64:\n",
        "        try:\n",
        "            image_data = base64.b64decode(image_base64)\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": f\"Failed to process image: {str(e)}\"}), 400\n",
        "    else:\n",
        "        image = None\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        # Pass both text and image as input to the model, if image exists\n",
        "        inputs = [message]\n",
        "        if image:\n",
        "            inputs.append(image)\n",
        "\n",
        "        # Using `stream=True` to handle the generation in streaming mode\n",
        "        response = model.generate_content(inputs, stream=True)\n",
        "        response.resolve()\n",
        "        context_text = str(response.text)\n",
        "        markdown_response = to_markdown(context_text)  # Format response as Markdown\n",
        "        return jsonify({\"response\": markdown_response})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Failed to generate response: {str(e)}\"}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=port)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WctrPAUfAZtZ",
        "outputId": "df91ebea-2c09-49e2-d98c-e1a51a3455d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu\n",
        "!pip install -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msQCsQ3pAZw6",
        "outputId": "eb3a72f5-2e69-4071-a135-0af9fd3d632f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.43.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.16)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.38)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.116)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<0.113.0 (from gradio)\n",
            "  Downloading fastapi-0.112.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<0.113.0->gradio)\n",
            "  Downloading starlette-0.38.4-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.43.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.112.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading starlette-0.38.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, ffmpy, aiofiles, starlette, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.112.4 ffmpy-0.4.0 gradio-4.43.0 gradio-client-1.3.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.4 semantic-version-2.10.0 starlette-0.38.4 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3qqrjpxAZ3G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from transformers import pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-p_sYSdAZ6m",
        "outputId": "0af52f45-62cd-44cf-e468-2680d49a1de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.116)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVFzoADMAj3U",
        "outputId": "30d8cad5-435b-4836-8586-8a7cd2d5c621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFZF4tYRAj6e"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx33OM5c-nJw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the YOLO object detection model\n",
        "inference_ = YOLO(\"/content/drive/MyDrive/WMS/best.pt\")  # Load your custom-trained model\n",
        "\n",
        "# GOOGLE_API_KEY = getpass()\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/pituitary-tumors-brochure.pdf\")\n",
        "docs = loader.load()\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "bnb_config = BitsAndBytesConfig(\n",
        " load_in_4bit=True,\n",
        " bnb_4bit_use_double_quant=True,\n",
        " bnb_4bit_quant_type=\"nf4\",\n",
        " bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "\n",
        "\n",
        "# Function to format text to Markdown (as a plain string)\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyBzOy5pGfDWfgTkd7pVgDRQJ5U35EXDvi8\")\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"Hello from Colab!\"\n",
        "\n",
        "# Object detection function\n",
        "@app.route(\"/object-detection\", methods=[\"POST\"])\n",
        "def object_detection():\n",
        "    print(\"Received a request for object detection\")\n",
        "    data = request.json\n",
        "\n",
        "    image_base64 = data.get(\"image\", \"\")\n",
        "\n",
        "    if not image_base64:\n",
        "        return jsonify({\"error\": \"No image provided\"}), 400\n",
        "\n",
        "    try:\n",
        "        # Decode the image from base64\n",
        "        image_data = base64.b64decode(image_base64)\n",
        "        image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "        # Convert PIL image to numpy array\n",
        "        img_np = np.array(image)\n",
        "\n",
        "        # Apply YOLO object detection\n",
        "        results = inference_([img_np])\n",
        "\n",
        "        # Extract the prediction and plot the results\n",
        "        for r in results:\n",
        "            im_array = r.plot()  # BGR numpy array of predictions\n",
        "            im = Image.fromarray(im_array[..., ::-1])  # Convert to RGB PIL image\n",
        "\n",
        "        # Convert the output image back to base64 to send it in response\n",
        "        buffered = io.BytesIO()\n",
        "        im.save(buffered, format=\"JPEG\")\n",
        "        img_base64_output = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        return jsonify({\"detected_image\": img_base64_output})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Failed to process image for object detection: {str(e)}\"}), 500\n",
        "\n",
        "@app.route(\"/send-message\", methods=[\"POST\"])\n",
        "def send_message():\n",
        "    print(\"Received a request\")\n",
        "    data = request.json\n",
        "\n",
        "    message = data.get(\"message\", \"\")\n",
        "    image_base64 = data.get(\"image\", \"\")\n",
        "\n",
        "    if not message and not image_base64:\n",
        "        return jsonify({\"error\": \"No message or image provided\"}), 400\n",
        "\n",
        "    # If there's an image, decode it from base64\n",
        "    if image_base64:\n",
        "        try:\n",
        "            image_data = base64.b64decode(image_base64)\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "        except Exception as e:\n",
        "            return jsonify({\"error\": f\"Failed to process image: {str(e)}\"}), 400\n",
        "    else:\n",
        "        image = None\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        # Pass both text and image as input to the model, if image exists\n",
        "        inputs = [message]\n",
        "        if image:\n",
        "            inputs.append(image)\n",
        "\n",
        "        # Using `stream=True` to handle the generation in streaming mode\n",
        "        response = model.generate_content(inputs, stream=True)\n",
        "        response.resolve()\n",
        "        context_text = str(response.text)\n",
        "        markdown_response = to_markdown(context_text)  # Format response as Markdown\n",
        "        prompt_template = \"\"\"\n",
        "        <|system|>\n",
        "        Answer the question based on your knowledge. Use the following information:\n",
        "\n",
        "        {markdown_response}\n",
        "\n",
        "\n",
        "        <|user|>\n",
        "        {question}\n",
        "        <|assistant|>\n",
        "\n",
        "         \"\"\"\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"markdown_response\", \"question\"],\n",
        "            template=prompt_template,\n",
        "        )\n",
        "\n",
        "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "        from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "        retriever = db.as_retriever()\n",
        "\n",
        "        rag_chain = (\n",
        "         {\"markdown_response\": retriever, \"question\": RunnablePassthrough()}\n",
        "            | llm_chain\n",
        "        )\n",
        "        output=rag_chain.invoke(message)['text']\n",
        "\n",
        "        return jsonify({\"response\": output})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Failed to generate response: {str(e)}\"}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=port)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEq1xBbRZpNT"
      },
      "source": [
        "with api key in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h7O8hblZsNj",
        "outputId": "ff7225ae-6027-433c-c8e2-939f1f1b1afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n",
            " * ngrok tunnel \"https://9d4f-35-247-156-102.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:10:33] \"\u001b[33mPOST /predict HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:11:35] \"\u001b[33mPOST /predict HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 17:13:02] \"\u001b[31m\u001b[1mPOST /send-message HTTP/1.1\u001b[0m\" 403 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Received a request\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from getpass import getpass\n",
        "import google.generativeai as genai\n",
        "from pyngrok import ngrok\n",
        "import textwrap\n",
        "\n",
        "# Secure API key\n",
        "API_KEY = \"my-super-secret-key\"  # Replace this with your own secure key\n",
        "GOOGLE_API_KEY = getpass()\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return \"Hello from Colab!\"\n",
        "\n",
        "@app.route(\"/send-message\", methods=[\"POST\"])\n",
        "def send_message():\n",
        "    print(\"Received a request\")\n",
        "\n",
        "    # Check for API key in the Authorization header\n",
        "    api_key = request.headers.get(\"Authorization\")\n",
        "    if api_key != f\"Bearer {API_KEY}\":\n",
        "        return jsonify({\"error\": \"Invalid API Key\"}), 403\n",
        "\n",
        "    data = request.json\n",
        "    print(\"Received data:\", data)\n",
        "\n",
        "    message = data.get(\"message\", \"\")\n",
        "    if not message:\n",
        "        return jsonify({\"error\": \"No message provided\"}), 400\n",
        "\n",
        "    print(\"Generating response...\")\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    response = model.generate_content(message)\n",
        "    print(\"Response generated.\")\n",
        "\n",
        "    response_text = response.text\n",
        "    if response_text is not None:\n",
        "        markdown_response = to_markdown(response_text)\n",
        "        return jsonify({\"response\": markdown_response})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Failed to generate a response\"}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(port=port)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF8IbY6uabBQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8oIzdKHETPG"
      },
      "source": [
        "improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht30NwrEETPG"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    YOLO_MODEL_PATH = \"/content/drive/MyDrive/WMS/best.pt\"\n",
        "    PDF_PATH = \"/content/pituitary-tumors-brochure.pdf\"\n",
        "    GEMINI_API_KEY = \"AIzaSyBzOy5pGfDWfgTkd7pVgDRQJ5U35EXDvi8\"\n",
        "    PORT = \"5000\"\n",
        "\n",
        "# object_detection.py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class ObjectDetector:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = YOLO(model_path)\n",
        "\n",
        "    def detect(self, image_base64):\n",
        "        try:\n",
        "            image_data = base64.b64decode(image_base64)\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "            img_np = np.array(image)\n",
        "            results = self.model([img_np])\n",
        "\n",
        "            for r in results:\n",
        "                im_array = r.plot()\n",
        "                im = Image.fromarray(im_array[..., ::-1])\n",
        "\n",
        "            buffered = io.BytesIO()\n",
        "            im.save(buffered, format=\"JPEG\")\n",
        "            return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to process image for object detection: {str(e)}\")\n",
        "\n",
        "# llm_chain.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "\n",
        "class LLMChain:\n",
        "    def __init__(self, pdf_path, model_name='HuggingFaceH4/zephyr-7b-beta'):\n",
        "        self.db = self._initialize_vector_store(pdf_path)\n",
        "        self.llm = self._initialize_llm(model_name)\n",
        "        self.rag_chain = self._setup_rag_chain()\n",
        "\n",
        "    def _initialize_vector_store(self, pdf_path):\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        docs = loader.load()\n",
        "        splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
        "        chunked_docs = splitter.split_documents(docs)\n",
        "        return FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))\n",
        "\n",
        "    def _initialize_llm(self, model_name):\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        text_generation_pipeline = pipeline(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            task=\"text-generation\",\n",
        "            temperature=0.2,\n",
        "            repetition_penalty=1.1,\n",
        "            return_full_text=True,\n",
        "            max_new_tokens=1000,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "        return HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "    def _setup_rag_chain(self):\n",
        "        prompt_template = \"\"\"\n",
        "        <|system|>\n",
        "        Answer the question based on your knowledge. Use the following information:\n",
        "\n",
        "        {markdown_response}\n",
        "\n",
        "        <|user|>\n",
        "        {question}\n",
        "        <|assistant|>\n",
        "        \"\"\"\n",
        "        prompt = PromptTemplate(input_variables=[\"markdown_response\", \"question\"], template=prompt_template)\n",
        "        llm_chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        retriever = self.db.as_retriever()\n",
        "        return (\n",
        "            {\"markdown_response\": retriever, \"question\": RunnablePassthrough()}\n",
        "            | llm_chain\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def to_markdown(text):\n",
        "        text = text.replace('•', '  *')\n",
        "        return textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "\n",
        "    def process_message(self, message, image=None):\n",
        "        try:\n",
        "            model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "            inputs = [message]\n",
        "            if image:\n",
        "                inputs.append(image)\n",
        "            response = model.generate_content(inputs, stream=True)\n",
        "            response.resolve()\n",
        "            context_text = str(response.text)\n",
        "            markdown_response = self.to_markdown(context_text)\n",
        "            output = self.rag_chain.invoke(message)['text']\n",
        "            return output\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to generate response: {str(e)}\")\n",
        "\n",
        "# app.py\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from config import Config\n",
        "from object_detection import ObjectDetector\n",
        "from llm_chain import LLMChain\n",
        "import google.generativeai as genai\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "\n",
        "def create_app(config_class=Config):\n",
        "    app = Flask(__name__)\n",
        "    app.config.from_object(config_class)\n",
        "\n",
        "    genai.configure(api_key=app.config['GEMINI_API_KEY'])\n",
        "    object_detector = ObjectDetector(app.config['YOLO_MODEL_PATH'])\n",
        "    llm_chain = LLMChain(app.config['PDF_PATH'])\n",
        "\n",
        "    @app.route(\"/\")\n",
        "    def index():\n",
        "        return \"Hello from Colab!\"\n",
        "\n",
        "    @app.route(\"/object-detection\", methods=[\"POST\"])\n",
        "    def object_detection():\n",
        "        data = request.json\n",
        "        image_base64 = data.get(\"image\", \"\")\n",
        "        if not image_base64:\n",
        "            return jsonify({\"error\": \"No image provided\"}), 400\n",
        "        try:\n",
        "            detected_image = object_detector.detect(image_base64)\n",
        "            return jsonify({\"detected_image\": detected_image})\n",
        "        except ValueError as e:\n",
        "            return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "    @app.route(\"/send-message\", methods=[\"POST\"])\n",
        "    def send_message():\n",
        "        data = request.json\n",
        "        message = data.get(\"message\", \"\")\n",
        "        image_base64 = data.get(\"image\", \"\")\n",
        "        if not message and not image_base64:\n",
        "            return jsonify({\"error\": \"No message or image provided\"}), 400\n",
        "        try:\n",
        "            image = None\n",
        "            if image_base64:\n",
        "                image_data = base64.b64decode(image_base64)\n",
        "                image = Image.open(io.BytesIO(image_data))\n",
        "            output = llm_chain.process_message(message, image)\n",
        "            return jsonify({\"response\": output})\n",
        "        except ValueError as e:\n",
        "            return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "    return app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_app()\n",
        "    public_url = ngrok.connect(app.config['PORT']).public_url\n",
        "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{app.config['PORT']}\\\"\")\n",
        "    app.config[\"BASE_URL\"] = public_url\n",
        "    app.run(port=app.config['PORT'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}